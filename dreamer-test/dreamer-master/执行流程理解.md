#### 调用 `static_scan` 的详细执行过程及形状分析

```python
post, prior = tools.static_scan(lambda prev, inputs: self.obs_step(prev[0], *inputs),(action, embed), (state, state))
```

#### 写在前面的：
##### 参数传递
- `fn`：`lambda prev, inputs: self.obs_step(prev[0], *inputs)`
    - `prev`：上一时刻的状态（`(post, prior)` 元组）
    - `inputs`：当前时间步的输入（`(action, embed)` 元组）
    - 调用 `self.obs_step` 结合动作和观测更新状态
- `inputs`：`(action, embed)`
    - `action`：动作序列，形状 `[time, batch, action_dim]`
    - `embed`：观测编码序列，形状 `[time, batch, embed_dim]`
- `start`：`(state, state)`
    - 初始状态的元组，用于初始化 `post` 和 `prior`


我们来详细分析 `post, prior = tools.static_scan(...)` 这一调用的执行流程和中间结果形状。假设输入参数如下：
```python
# 假设输入参数
action.shape = [time=3, batch=2, action_dim=4]  # 3个时间步，2个样本，动作维度4
embed.shape = [time=3, batch=2, embed_dim=5]   # 3个时间步，2个样本，观测维度5
state = {
    'stoch': tf.zeros([batch=2, stoch_size=30]),  # 随机状态
    'deter': tf.zeros([batch=2, deter_size=200])  # 确定性状态
}
```
#### 1. 初始化阶段
```python
last = (state, state)  # 初始状态，元组 (post, prior)
```
##### 形状：

- `state['stoch']: [2, 30]`
- `state['deter']: [2, 200]`

#### 2. 创建输出列表
```python
outputs = [[] for _ in tf.nest.flatten(start)]
```
##### 结构
```python
outputs = [
    [],  # 对应 state['stoch']（post）
    [],  # 对应 state['deter']（post）
    [],  # 对应 state['stoch']（prior）
    [],  # 对应 state['deter']（prior）
]
```
#### 3. 确定序列长度
```python
indices = range(len(tf.nest.flatten(inputs)[0]))  # [0, 1, 2]
```
#### 4. 循环处理每个时间步
##### 时间步 t=0
```python
# 获取当前输入
action_0 = action[0]  # 形状: [2, 4]
embed_0 = embed[0]    # 形状: [2, 5]

# 调用 obs_step
post_0, prior_0 = self.obs_step(state, action_0, embed_0)
```

##### `obs_step` 内部形状变化：

`prev_state['stoch']` + `prev_action` → 拼接后形状：`[2, 30+4]` = `[2, 34]`
经过 **GRU** 后，`deter` 形状：`[2, 200]`
最终 `stoch` 形状：`[2, 30]`

##### 输出形状：

- `post_0`:
    - `post_0['stoch']`: `[2, 30]`
    - `post_0['deter']`: `[2, 200]`
- `prior_0`: 类似 `post_0`

##### 更新输出列表：
```python 
outputs = [
    [post_0['stoch']],  # 形状: [2, 30]
    [post_0['deter']],  # 形状: [2, 200]
    [prior_0['stoch']], # 形状: [2, 30]
    [prior_0['deter']], # 形状: [2, 200]
]
```

##### 时间步 t=1
```python
# 获取当前输入
action_1 = action[1]  # 形状: [2, 4]
embed_1 = embed[1]    # 形状: [2, 5]

# 调用 obs_step，传入上一步的 post_0
post_1, prior_1 = self.obs_step(post_0, action_1, embed_1) 
# 这里能看出 prev[0] 的作用（取 post_0 ，而没有取 prior_0）
```
##### 更新输出列表：
```python 
outputs = [
    [post_0['stoch'], post_1['stoch']],  # 2个元素，每个 [2, 30]
    [post_0['deter'], post_1['deter']],  # 2个元素，每个 [2, 200]
    [prior_0['stoch'], prior_1['stoch']], # 2个元素，每个 [2, 30]
    [prior_0['deter'], prior_1['deter']], # 2个元素，每个 [2, 200]
]
```

##### 时间步 t=2
类似 **t=1**，最终 `outputs` 包含每个时间步的状态：
```python 
outputs = [
    [post_0['stoch'], post_1['stoch'], post_2['stoch']],  # 3个元素，每个 [2, 30]
    [post_0['deter'], post_1['deter'], post_2['deter']],  # 3个元素，每个 [2, 200]
    [prior_0['stoch'], prior_1['stoch'], prior_2['stoch']], # 3个元素，每个 [2, 30]
    [prior_0['deter'], prior_1['deter'], prior_2['deter']], # 3个元素，每个 [2, 200]
]
```
#### 5. 后处理阶段
##### 堆叠张量
```python
outputs = [tf.stack(x, 0) for x in outputs]
```
##### 形状：
```python
outputs = [
    [3, 2, 30],  # post['stoch']
    [3, 2, 200], # post['deter']
    [3, 2, 30],  # prior['stoch']
    [3, 2, 200], # prior['deter']
]
```
##### 恢复嵌套结构
```python
return tf.nest.pack_sequence_as(start, outputs)
```
最终输出形状：

- `post`:
    - `post['stoch']`: `[3, 2, 30]`
    - `post['deter']`: `[3, 2, 200]`
- `prior`:
    - `prior['stoch']`: `[3, 2, 30]`
    - `prior['deter']`: `[3, 2, 200]`

##### 补充：
| 操作       | 传递给 `obs_step` 的参数          | 结果                     |
|------------|----------------------------------|--------------------------|
| `inputs`   | `prev_state, (action_t, embed_t)` | 错误：参数数量不匹配       |
| `*inputs`  | `prev_state, action_t, embed_t`   | 正确：参数数量匹配         |